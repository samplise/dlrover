From 92e2ad2590db6e1e640bb363aad54046efda9f73 Mon Sep 17 00:00:00 2001
From: zj <1023086844@qq.com>
Date: Wed, 3 Sep 2025 10:54:41 +0000
Subject: [PATCH] feat(exp): add bad sync

---
 BAD_SYNC.md     |  14 ++++
 pretrain_gpt.py |   3 +-
 train_llama.sh  | 178 ++++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 194 insertions(+), 1 deletion(-)
 create mode 100644 BAD_SYNC.md
 create mode 100755 train_llama.sh

diff --git a/BAD_SYNC.md b/BAD_SYNC.md
new file mode 100644
index 00000000..a182b5d7
--- /dev/null
+++ b/BAD_SYNC.md
@@ -0,0 +1,14 @@
+# megatron bad sync
+
+enable bad sync, 8xA100 194.5TFLOPS
+
+```bash
+BAD_SYNC=1 bash train_llama.sh
+```
+
+disable bad sync, 8xA100 196.2TFLOPS
+
+
+```bash
+train_llama.sh
+```
diff --git a/pretrain_gpt.py b/pretrain_gpt.py
index 0bd85b76..eb0d4e7a 100644
--- a/pretrain_gpt.py
+++ b/pretrain_gpt.py
@@ -34,6 +34,7 @@ from megatron.core.models.gpt.gpt_layer_specs import (
     get_gpt_layer_with_transformer_engine_spec,
 )
 
+bad_sync_level = 0 if "BAD_SYNC" in os.environ else 2
 
 stimer = StragglerDetector()
 
@@ -181,7 +182,7 @@ def forward_step(data_iterator, model: GPTModel):
     timers = get_timers()
 
     # Get the batch.
-    timers('batch-generator', log_level=2).start()
+    timers('batch-generator', log_level=bad_sync_level).start()
     global stimer
     with stimer(bdata=True):
         tokens, labels, loss_mask, attention_mask, position_ids = get_batch(
diff --git a/train_llama.sh b/train_llama.sh
new file mode 100755
index 00000000..25b3abb1
--- /dev/null
+++ b/train_llama.sh
@@ -0,0 +1,178 @@
+
+#!/bin/bash
+
+# Environment variables for performance tuning
+export CUDA_DEVICE_MAX_CONNECTIONS=${CUDA_DEVICE_MAX_CONNECTIONS:-1}
+
+CHECKPOINT_PATH=${1:-"checkpoints/llama3_8b_fp8"}
+TENSORBOARD_LOGS_PATH=${2:-"tensorboard_logs/llama3_8b_fp8"}
+TOKENIZER_ARG=${3:-"MOCK"} # Path to tokenizer model, or "MOCK"
+DATA_ARG=${4:-"MOCK"}     # Data prefix, or "MOCK"
+
+# Create directories if they don't exist
+mkdir -p "$(dirname "$CHECKPOINT_PATH")"
+mkdir -p "$(dirname "$TENSORBOARD_LOGS_PATH")"
+
+# Distributed training setup
+GPUS_PER_NODE=8
+NUM_NODES=1
+MASTER_ADDR=${MASTER_ADDR:-localhost}
+MASTER_PORT=${MASTER_PORT:-6000}
+NODE_RANK=${NODE_RANK:-0}
+WORLD_SIZE=$(($GPUS_PER_NODE*$NUM_NODES))
+
+# Path to the pretrain_gpt.py script, assuming this script is run from the root of the Megatron-LM repository
+PRETRAIN_SCRIPT_PATH="pretrain_gpt.py"
+
+# Fixed model and training parameters
+TP_SIZE=4
+CP_SIZE=1
+PP_SIZE=1
+MICRO_BATCH_SIZE=1
+GLOBAL_BATCH_SIZE=32
+NUM_LAYERS=2
+DTYPE="bf16"
+SEQ_LENGTH=4096
+MAX_POSITION_EMBEDDINGS=4096
+
+# Data cache path (useful for both mock and real data)
+DATA_CACHE_PATH="${PWD}/benchmark_cache_llama3_8b_fp8"
+mkdir -p "$DATA_CACHE_PATH"
+
+DISTRIBUTED_ARGS=(
+    --nproc_per_node $GPUS_PER_NODE
+    --nnodes $NUM_NODES
+    --node_rank $NODE_RANK
+    --master_addr $MASTER_ADDR
+    --master_port $MASTER_PORT
+)
+
+MODEL_ARGS=(
+    --use-mcore-models
+    --num-layers $NUM_LAYERS
+    --hidden-size 8192
+    --ffn-hidden-size 29568
+    --num-attention-heads 64
+    --group-query-attention
+    --num-query-groups 8
+    --kv-channels 128
+    --seq-length $SEQ_LENGTH
+    --max-position-embeddings $MAX_POSITION_EMBEDDINGS
+    --position-embedding-type rope
+    --rotary-base 1000000
+    --rotary-percent 1.0
+    --attention-dropout 0.0
+    --hidden-dropout 0.0
+    --swiglu
+    --untie-embeddings-and-output-weights
+    --use-flash-attn
+    --disable-bias-linear
+    --normalization RMSNorm
+)
+
+TRAINING_ARGS=(
+    --micro-batch-size $MICRO_BATCH_SIZE
+    --global-batch-size $GLOBAL_BATCH_SIZE
+    --train-iters 100
+    --lr 0.00015
+    --min-lr 0.00001
+    --decoupled-lr 5.0e-4      # Specific to decoupled AdamW, ensure optimizer is compatible
+    --decoupled-min-lr 4.5e-5  # Specific to decoupled AdamW
+    --lr-decay-style cosine
+    --clip-grad 1.0
+    --weight-decay 0.1
+    --adam-beta1 0.9
+    --adam-beta2 0.95
+    --bf16
+    --manual-gc
+    --manual-gc-interval 10
+)
+
+# Conditional arguments based on DTYPE (FP8)
+DTYPE_ARGS=()
+if [[ "$DTYPE" == "fp8" ]]; then
+    DTYPE_ARGS+=(
+        "--fp8-format hybrid"
+        "--fp8-amax-history-len 1024"
+        "--fp8-amax-compute-algo max"
+        "--fp8-param-gather"
+    )
+fi
+
+# Model parallelism arguments
+MODEL_PARALLEL_ARGS=(
+    --tensor-model-parallel-size $TP_SIZE
+    --context-parallel-size $CP_SIZE
+    # --pipeline-model-parallel-size $PP_SIZE # Not explicitly set in llama script options, assume 1 if not multi-node PP
+    --sequence-parallel  # Always enable sequence parallelism with TP_SIZE=2
+)
+
+# Distributed Data Parallel (DDP) arguments
+# From original script's ddp_args
+DDP_ARGS=(
+    --use-distributed-optimizer
+    --overlap-grad-reduce
+    --overlap-param-gather
+)
+TRAINING_ARGS+=("${DDP_ARGS[@]}")
+
+
+# Data arguments (conditional for mock vs real data)
+DATA_ARGS_LIST=()
+if [[ "$TOKENIZER_ARG" == "MOCK" ]] || [[ "$DATA_ARG" == "MOCK" ]] || [[ -z "$TOKENIZER_ARG" ]]; then
+    DATA_ARGS_LIST+=(
+        "--mock-data"
+        "--tokenizer-type NullTokenizer"
+        "--vocab-size 128256"
+        "--data-cache-path ${DATA_CACHE_PATH}"
+        "--tiktoken-pattern v2"
+        "--split '99,1,0'"
+        "--no-create-attention-mask-in-dataloader"
+        "--no-mmap-bin-files"
+        "--num-workers 1"
+    )
+else
+    # Settings for real data
+    DATA_ARGS_LIST+=(
+        "--data-path $DATA_ARG"
+        "--tokenizer-type HuggingFaceTokenizer"
+        "--tokenizer-model $TOKENIZER_ARG"
+        "--data-cache-path ${DATA_CACHE_PATH}"
+        "--split '99,1,0'"
+        "--no-create-attention-mask-in-dataloader"
+        "--no-mmap-bin-files"
+        "--num-workers 1"
+        # Note: --vocab-size might be inferred by HuggingFaceTokenizer or might need to be explicit.
+        "--vocab-size 128256"
+    )
+fi
+
+EVAL_AND_LOGGING_ARGS=(
+    --log-interval 1
+    --eval-iters 32
+    --eval-interval 100
+    --save-interval 1000
+    --log-throughput
+    --ckpt-format torch_dist
+    --distributed-timeout-minutes 60
+    --save "$CHECKPOINT_PATH"
+)
+
+# Ensure pretrain_gpt.py is found
+if [ ! -f "$PRETRAIN_SCRIPT_PATH" ]; then
+    echo "Error: pretrain_gpt.py not found at $PRETRAIN_SCRIPT_PATH"
+    echo "Please ensure you are running this script from the root of the Megatron-LM repository, and pretrain_gpt.py is present."
+    exit 1
+fi
+
+# Run the training command
+torchrun ${DISTRIBUTED_ARGS[@]} \
+    "$PRETRAIN_SCRIPT_PATH" \
+    ${MODEL_ARGS[@]} \
+    ${TRAINING_ARGS[@]} \
+    ${DTYPE_ARGS[@]} \
+    ${MODEL_PARALLEL_ARGS[@]} \
+    ${DATA_ARGS_LIST[@]} \
+    ${EVAL_AND_LOGGING_ARGS[@]}
+
+set +x
-- 
2.43.0

